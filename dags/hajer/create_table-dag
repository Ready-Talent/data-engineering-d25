from airflow import DAG
from airflow.providers.google.cloud.operators.bigquery import BigQueryCreateEmptyTableOperator
from airflow.providers.google.cloud.operators.bigquery import BigQueryInsertJobOperator

default_args = {
    'retries': 1
}

with DAG(
    'dim_customer_hajer',
    describtion='creating dim_customer dag',
    default_args=default_args,
    schedule_interval=None,
) as dag:
    
    create_dim_customers = BigQueryCreateEmptyTableOperator(
        task_id='create_table',
        dataset_id='airflow_star_schema',
        table_id='dim_customers_hajer',
        schema_fields=[
            {
                "name": "customer_id",
                "mode": "",
                "type": "INTEGER"
            },
            {
                "name": "name",
                "mode": "",
                "type": "STRING"
            },
            {
                "name": "email",
                "mode": "",
                "type": "STRING"
            },
            {
                "name": "address",
                "mode": "",
                "type": "STRING"
            },
            {
                "name": "phone",
                "mode": "",
                "type": "STRING"
            },
            {
                "name": "created_at",
                "mode": "",
                "type": "DATE"
            },
            {
                "name": "updated_at",
                "mode": "",
                "type": "DATE"
            }
            ]
    )



    populate_dim_customers = BigQueryInsertJobOperator(
        task_id='insert_query_job',
        configuration={
            "query": {
                "query": 'insert into ready-de-25.airflow_star_schema.dim_customers_hajer as (select * from ready-de-25.ecommerce.dim_customer)',
                "useLegacySql": False,
            }
        }
    )

    create_dim_customers >> populate_dim_customers